
create network vocab_size 21388 output_size 21388 embedding_dim 256 hidden_dim 256 n_layers 2
Training for 10 epoch(s)...
2020-04-15 10:01:25.110164 epoch 1
2020-04-15 10:02:38.752266 Batch 1000 of 3484 Epoch:    1/10    Loss: 5.104982435703278

2020-04-15 10:03:52.316937 Batch 2000 of 3484 Epoch:    1/10    Loss: 4.450939106941223

2020-04-15 10:05:05.912458 Batch 3000 of 3484 Epoch:    1/10    Loss: 4.272383866310119

2020-04-15 10:05:41.587282 epoch 2
2020-04-15 10:06:57.597514 Batch 1000 of 3484 Epoch:    2/10    Loss: 4.0915450045683315

2020-04-15 10:08:13.420900 Batch 2000 of 3484 Epoch:    2/10    Loss: 3.992008668899536

2020-04-15 10:09:29.235413 Batch 3000 of 3484 Epoch:    2/10    Loss: 3.953021254301071

2020-04-15 10:10:05.808906 epoch 3
2020-04-15 10:11:19.379489 Batch 1000 of 3484 Epoch:    3/10    Loss: 3.8392552606500385

2020-04-15 10:12:32.905116 Batch 2000 of 3484 Epoch:    3/10    Loss: 3.795134864330292

2020-04-15 10:13:46.414848 Batch 3000 of 3484 Epoch:    3/10    Loss: 3.780463198661804

2020-04-15 10:14:22.043443 epoch 4
2020-04-15 10:15:37.642063 Batch 1000 of 3484 Epoch:    4/10    Loss: 3.681465454821317



create network vocab_size 21388 output_size 21388 embedding_dim 512 hidden_dim 256 n_layers 2
Training for 10 epoch(s)...
2020-04-15 10:17:16.492615 epoch 1
2020-04-15 10:18:37.947736 Batch 1000 of 3484 Epoch:    1/10    Loss: 5.044202900409698

2020-04-15 10:19:59.198870 Batch 2000 of 3484 Epoch:    1/10    Loss: 4.421189313411713

2020-04-15 10:21:20.951310 Batch 3000 of 3484 Epoch:    1/10    Loss: 4.260049055337906

2020-04-15 10:22:00.720161 epoch 2
2020-04-15 10:23:20.056881 Batch 1000 of 3484 Epoch:    2/10    Loss: 4.077144330563249

2020-04-15 10:24:39.363616 Batch 2000 of 3484 Epoch:    2/10    Loss: 3.976019988298416

2020-04-15 10:25:58.686201 Batch 3000 of 3484 Epoch:    2/10    Loss: 3.9356987118721007

2020-04-15 10:26:37.119072 epoch 3
2020-04-15 10:27:58.645862 Batch 1000 of 3484 Epoch:    3/10    Loss: 3.8341190792479605

2020-04-15 10:29:20.064419 Batch 2000 of 3484 Epoch:    3/10    Loss: 3.784014576911926

2020-04-15 10:30:41.439744 Batch 3000 of 3484 Epoch:    3/10    Loss: 3.7807559237480164

2020-04-15 10:31:20.810933 epoch 4
2020-04-15 10:32:40.166002 Batch 1000 of 3484 Epoch:    4/10    Loss: 3.6792569117083382

2020-04-15 10:33:59.548640 Batch 2000 of 3484 Epoch:    4/10    Loss: 3.6582133588790895

create network vocab_size 21388 output_size 21388 embedding_dim 256 hidden_dim 512 n_layers 2
Training for 10 epoch(s)...
2020-04-15 10:34:44.414535 epoch 1
2020-04-15 10:36:04.577320 Batch 1000 of 3484 Epoch:    1/10    Loss: 4.8443652002811435

2020-04-15 10:37:24.595862 Batch 2000 of 3484 Epoch:    1/10    Loss: 4.300497798442841

2020-04-15 10:38:44.640459 Batch 3000 of 3484 Epoch:    1/10    Loss: 4.156637112617493

2020-04-15 10:39:23.408178 epoch 2
2020-04-15 10:40:43.763813 Batch 1000 of 3484 Epoch:    2/10    Loss: 3.958814903089621

2020-04-15 10:42:04.034969 Batch 2000 of 3484 Epoch:    2/10    Loss: 3.8699718930721283

2020-04-15 10:43:24.278605 Batch 3000 of 3484 Epoch:    2/10    Loss: 3.8405748081207274

2020-04-15 10:44:03.146341 epoch 3
2020-04-15 10:45:23.463125 Batch 1000 of 3484 Epoch:    3/10    Loss: 3.711069808012713

2020-04-15 10:46:43.872363 Batch 2000 of 3484 Epoch:    3/10    Loss: 3.658473265647888

2020-04-15 10:48:04.297005 Batch 3000 of 3484 Epoch:    3/10    Loss: 3.650670877456665

2020-04-15 10:48:43.161661 epoch 4


create network vocab_size 21388 output_size 21388 embedding_dim 256 hidden_dim 512 n_layers 3
Training for 10 epoch(s)...
2020-04-15 11:58:33.907929 epoch 1
2020-04-15 12:00:07.659013 Batch 1000 of 3484 Epoch:    1/10    Loss: 5.616705647945404

2020-04-15 12:01:41.993818 Batch 2000 of 3484 Epoch:    1/10    Loss: 4.643994634389878

2020-04-15 12:03:16.411668 Batch 3000 of 3484 Epoch:    1/10    Loss: 4.386493120908737

2020-04-15 12:04:01.919072 epoch 2
2020-04-15 12:05:34.641974 Batch 1000 of 3484 Epoch:    2/10    Loss: 4.197762897554433

2020-04-15 12:07:07.399801 Batch 2000 of 3484 Epoch:    2/10    Loss: 4.083615350484848

2020-04-15 12:08:40.155704 Batch 3000 of 3484 Epoch:    2/10    Loss: 4.056587496042251

2020-04-15 12:09:25.046246 epoch 3
2020-04-15 12:10:59.053536 Batch 1000 of 3484 Epoch:    3/10    Loss: 3.9342113293084817

2020-04-15 12:12:33.306386 Batch 2000 of 3484 Epoch:    3/10    Loss: 3.8632639491558076

2020-04-15 12:14:07.470094 Batch 3000 of 3484 Epoch:    3/10    Loss: 3.846524788379669

2020-04-15 12:14:53.085625 epoch 4
2020-04-15 12:16:25.789384 Batch 1000 of 3484 Epoch:    4/10    Loss: 3.7435845099690788

2020-04-15 12:17:58.526666 Batch 2000 of 3484 Epoch:    4/10    Loss: 3.7060750336647033

2020-04-15 12:19:31.278071 Batch 3000 of 3484 Epoch:    4/10    Loss: 3.7069732046127317

2020-04-15 12:20:16.132832 epoch 5
2020-04-15 12:21:49.755378 Batch 1000 of 3484 Epoch:    5/10    Loss: 3.613462059806299

2020-04-15 12:23:23.961701 Batch 2000 of 3484 Epoch:    5/10    Loss: 3.5731159904003142


create network vocab_size 21388 output_size 21388 embedding_dim 256 hidden_dim 1024 n_layers 2
Training for 5 epoch(s)...
2020-04-15 12:24:45.346079 epoch 1
2020-04-15 12:28:06.224605 Batch 1000 of 3484 Epoch:    1/5     Loss: 4.700392956733704

2020-04-15 12:31:26.978661 Batch 2000 of 3484 Epoch:    1/5     Loss: 4.188651426315308



create network vocab_size 21388 output_size 21388 embedding_dim 256 hidden_dim 128 n_layers 2
Training for 5 epoch(s)...
2020-04-15 12:33:33.210792 epoch 1
2020-04-15 12:34:09.111285 Batch 1000 of 3484 Epoch:    1/5     Loss: 5.238659876346588

2020-04-15 12:34:45.005798 Batch 2000 of 3484 Epoch:    1/5     Loss: 4.607111057281494

2020-04-15 12:35:21.042396 Batch 3000 of 3484 Epoch:    1/5     Loss: 4.420628143072128

2020-04-15 12:35:38.504177 epoch 2
2020-04-15 12:36:14.642506 Batch 1000 of 3484 Epoch:    2/5     Loss: 4.238122183839587

2020-04-15 12:36:50.753614 Batch 2000 of 3484 Epoch:    2/5     Loss: 4.14539897441864

2020-04-15 12:37:26.846293 Batch 3000 of 3484 Epoch:    2/5     Loss: 4.113033996343613

2020-04-15 12:37:44.321360 epoch 3
2020-04-15 12:38:20.521055 Batch 1000 of 3484 Epoch:    3/5     Loss: 4.015983072252608

2020-04-15 12:38:56.629100 Batch 2000 of 3484 Epoch:    3/5     Loss: 3.9655452439785

2020-04-15 12:39:32.725054 Batch 3000 of 3484 Epoch:    3/5     Loss: 3.966953863143921

2020-04-15 12:39:50.103555 epoch 4
2020-04-15 12:40:26.045995 Batch 1000 of 3484 Epoch:    4/5     Loss: 3.901592946116815

2020-04-15 12:41:02.155872 Batch 2000 of 3484 Epoch:    4/5     Loss: 3.859219804763794

2020-04-15 12:41:38.246909 Batch 3000 of 3484 Epoch:    4/5     Loss: 3.8652803075313567

2020-04-15 12:41:55.722213 epoch 5
2020-04-15 12:42:31.882511 Batch 1000 of 3484 Epoch:    5/5     Loss: 3.8004111361632127

2020-04-15 12:43:07.970937 Batch 2000 of 3484 Epoch:    5/5     Loss: 3.7869929721355438



create network vocab_size 21388 output_size 21388 embedding_dim 128 hidden_dim 256 n_layers 2
Training for 5 epoch(s)...
2020-04-15 12:43:52.612078 epoch 1
2020-04-15 12:44:41.632306 Batch 1000 of 3484 Epoch:    1/5     Loss: 5.2129301862716675

2020-04-15 12:45:30.553940 Batch 2000 of 3484 Epoch:    1/5     Loss: 4.554551338195801

2020-04-15 12:46:19.335165 Batch 3000 of 3484 Epoch:    1/5     Loss: 4.358039381742477

2020-04-15 12:46:42.966479 epoch 2
2020-04-15 12:47:31.925863 Batch 1000 of 3484 Epoch:    2/5     Loss: 4.168290304848448

2020-04-15 12:48:20.827013 Batch 2000 of 3484 Epoch:    2/5     Loss: 4.069844778776169

2020-04-15 12:49:09.759862 Batch 3000 of 3484 Epoch:    2/5     Loss: 4.034746732711792

2020-04-15 12:49:33.457110 epoch 3
2020-04-15 12:50:22.384194 Batch 1000 of 3484 Epoch:    3/5     Loss: 3.929419423049351

2020-04-15 12:51:11.228348 Batch 2000 of 3484 Epoch:    3/5     Loss: 3.8784764807224272

2020-04-15 12:51:59.973033 Batch 3000 of 3484 Epoch:    3/5     Loss: 3.855786018371582

2020-04-15 12:52:23.521102 epoch 4
2020-04-15 12:53:12.464687 Batch 1000 of 3484 Epoch:    4/5     Loss: 3.774597688344611

2020-04-15 12:54:01.361563 Batch 2000 of 3484 Epoch:    4/5     Loss: 3.7410393736362457

2020-04-15 12:54:50.205794 Batch 3000 of 3484 Epoch:    4/5     Loss: 3.7318322203159333

2020-04-15 12:55:13.866185 epoch 5
2020-04-15 12:56:02.832031 Batch 1000 of 3484 Epoch:    5/5     Loss: 3.6695999210735537

2020-04-15 12:56:51.699512 Batch 2000 of 3484 Epoch:    5/5     Loss: 3.6260268540382383

2020-04-15 12:57:40.535552 Batch 3000 of 3484 Epoch:    5/5     Loss: 3.6452021048069


create network vocab_size 21388 output_size 21388 embedding_dim 256 hidden_dim 256 n_layers 2
Training for 6 epoch(s)...
2020-04-15 13:04:22.619230 epoch 1
2020-04-15 13:05:13.233889 Batch 1000 of 3484 Epoch:    1/6     Loss: 5.069509801864624

2020-04-15 13:06:03.428207 Batch 2000 of 3484 Epoch:    1/6     Loss: 4.448899069309235

2020-04-15 13:06:53.613682 Batch 3000 of 3484 Epoch:    1/6     Loss: 4.2573348951339725

2020-04-15 13:07:17.910544 epoch 2
2020-04-15 13:08:08.405430 Batch 1000 of 3484 Epoch:    2/6     Loss: 4.087602934586712

2020-04-15 13:08:58.787257 Batch 2000 of 3484 Epoch:    2/6     Loss: 4.004778999328614

2020-04-15 13:09:49.171083 Batch 3000 of 3484 Epoch:    2/6     Loss: 3.9668686397075654

2020-04-15 13:10:13.572880 epoch 3
2020-04-15 13:11:04.052673 Batch 1000 of 3484 Epoch:    3/6     Loss: 3.853603081561806

2020-04-15 13:11:54.396634 Batch 2000 of 3484 Epoch:    3/6     Loss: 3.797958275794983

2020-04-15 13:12:44.800916 Batch 3000 of 3484 Epoch:    3/6     Loss: 3.805125911951065

2020-04-15 13:13:09.241336 epoch 4
2020-04-15 13:13:59.716629 Batch 1000 of 3484 Epoch:    4/6     Loss: 3.7066105401740885

2020-04-15 13:14:50.103517 Batch 2000 of 3484 Epoch:    4/6     Loss: 3.672935401916504

2020-04-15 13:15:40.585961 Batch 3000 of 3484 Epoch:    4/6     Loss: 3.684238198041916

2020-04-15 13:16:04.945839 epoch 5
2020-04-15 13:16:55.370642 Batch 1000 of 3484 Epoch:    5/6     Loss: 3.5990152346156035

2020-04-15 13:17:45.768263 Batch 2000 of 3484 Epoch:    5/6     Loss: 3.578118129491806

2020-04-15 13:18:36.154814 Batch 3000 of 3484 Epoch:    5/6     Loss: 3.5882999608516695

2020-04-15 13:19:00.552524 epoch 6
2020-04-15 13:19:50.977897 Batch 1000 of 3484 Epoch:    6/6     Loss: 3.514690157377495

2020-04-15 13:20:41.317129 Batch 2000 of 3484 Epoch:    6/6     Loss: 3.5021340019702913

2020-04-15 13:21:31.706183 Batch 3000 of 3484 Epoch:    6/6     Loss: 3.5190475573539732

